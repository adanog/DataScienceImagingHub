{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adanog/DataScienceImagingHub/blob/main/241016_Predict_Class_using_Trained_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c814820",
      "metadata": {
        "id": "0c814820"
      },
      "source": [
        "\n",
        "# About the notebook\n",
        "The purpose of this Jupyter Notebook is to use a pre-trained deep learning model to generate class predictions for a given input image.\n",
        "\n",
        "# References\n",
        "[1] HernÃ¡ndez-Herrera et. al. 2023, https://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/4772/3617"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad28b6b",
      "metadata": {
        "id": "3ad28b6b"
      },
      "source": [
        "# 00 - Special Instructions for Google Colab Users\n",
        "\n",
        "The following lines of code should be executed only when running your script on Google Colab. This is crucial to leverage the additional features provided by Colab, most notably, the availability of a free GPU.  **If, you're running the code locally, this line can be skipped (GO TO STEP 01 - Loading dependencies) as it pertains specifically to the Colab setup.**\n",
        "\n",
        "## Give access to google drive\n",
        "\n",
        "### If you have a shared folder just add a direct access from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8060f967",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8060f967",
        "outputId": "6380054b-5a04-4a6c-f8e4-b95a47393266",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7819107",
      "metadata": {
        "id": "d7819107"
      },
      "source": [
        "## Copy code to current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d5e9c7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d5e9c7e",
        "outputId": "5fdbf356-342f-4955-c03c-e92d0f1f4437",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'image_classification_pytorch'...\n",
            "remote: Enumerating objects: 202, done.\u001b[K\n",
            "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 202 (delta 112), reused 165 (delta 78), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (202/202), 765.11 KiB | 5.67 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/paul-hernandez-herrera/image_classification_pytorch\n",
        "import os\n",
        "workbookDir = \"/content/image_classification_pytorch/\"\n",
        "os.chdir(workbookDir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9fa4ea",
      "metadata": {
        "id": "2d9fa4ea"
      },
      "source": [
        "# 01 - Loading dependencies\n",
        "In this notebook, before running any code, there are several libraries and modules that need to be imported to ensure that the notebook runs smoothly. These libraries and modules contain pre-written code that performs specific tasks, such as reading and processing images, defining the UNET model, and training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d18617ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d18617ce",
        "outputId": "d1397c47-47a3-47d3-85ce-db4700727f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/image_classification_pytorch\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if the global variable 'workbookDir' is defined\n",
        "if 'workbookDir' not in globals():\n",
        "    print('Updating working directory')\n",
        "\n",
        "    # Get the directory name of the current working directory\n",
        "    workbookDir = os.path.dirname(os.getcwd())\n",
        "\n",
        "    # Change the current working directory to the one above\n",
        "    os.chdir(workbookDir)\n",
        "\n",
        "# Print the current working directory\n",
        "print(os.getcwd())\n",
        "\n",
        "# Import the PyTorch library for machine learning and deep learning\n",
        "import torch\n",
        "\n",
        "# Import custom classes/functions for predictions and image display\n",
        "from core_code.predict import PredictClassInteractive\n",
        "from core_code.util.show_image import show_images_predicted_class_interactive\n",
        "\n",
        "# Load the autoreload extension for Jupyter notebooks\n",
        "%load_ext autoreload\n",
        "\n",
        "# Enable autoreload for all modules before executing code\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01.5 - Create stacks from two channels, run only if necessary\n",
        "\n",
        "The model require one single image from 2 channels in tif format. This cell is create single images from to brightfield and fluorescence image to a single one."
      ],
      "metadata": {
        "id": "A4l3g1wM2-0v"
      },
      "id": "A4l3g1wM2-0v"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Pillow numpy tifffile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCUUp7evuxay",
        "outputId": "17ae23d7-cc61-4bf7-ec0a-3ba2fb98d094"
      },
      "id": "VCUUp7evuxay",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (2024.9.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "\n",
        "# Paths to the directories\n",
        "campo_claro_dir = '/content/drive/MyDrive/20241016_ImageClassifierFocus/Dataset/Canal campo claro/'\n",
        "fluorescencia_dir = '/content/drive/MyDrive/20241016_ImageClassifierFocus/Dataset/Canal fluorescencia'\n",
        "\n",
        "# Get all TIFF files from both directories\n",
        "campo_claro_files = sorted([os.path.join(campo_claro_dir, f) for f in os.listdir(campo_claro_dir) if f.endswith('.ome.tif')])\n",
        "fluorescencia_files = sorted([os.path.join(fluorescencia_dir, f) for f in os.listdir(fluorescencia_dir) if f.endswith('.ome.tif')])\n",
        "\n",
        "# Check that we have the same number of files in both directories\n",
        "if len(campo_claro_files) != len(fluorescencia_files):\n",
        "    raise ValueError(\"Number of images in 'Canal Campo Claro' and 'Canal Fluorescencia' must match.\")\n",
        "\n",
        "# Process each pair of files\n",
        "for i, (file1_path, file2_path) in enumerate(zip(campo_claro_files, fluorescencia_files)):\n",
        "    # Read the images using tifffile\n",
        "    image1 = tiff.imread(file1_path)\n",
        "    image2 = tiff.imread(file2_path)\n",
        "\n",
        "    # Ensure both images have the same size\n",
        "    if image1.shape != image2.shape:\n",
        "        raise ValueError(f\"Images {file1_path} and {file2_path} must be the same size to concatenate them as channels.\")\n",
        "\n",
        "    # Convert to appropriate shape if necessary\n",
        "    if image1.ndim == 3 and image1.shape[0] > 1:\n",
        "        image1 = image1.transpose(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
        "    if image2.ndim == 3 and image2.shape[0] > 1:\n",
        "        image2 = image2.transpose(1, 2, 0)\n",
        "\n",
        "    # Stack the images along a new channel dimension\n",
        "    concatenated = np.stack((image1, image2), axis=-1)  # Shape will be (H, W, C, 2)\n",
        "\n",
        "    # Convert to shape (C, H, W) for TIFF\n",
        "    concatenated = np.moveaxis(concatenated, -1, 0)  # Now shape is (2, H, W)\n",
        "\n",
        "    # Save the concatenated image\n",
        "    output_path = f'/content/drive/MyDrive/20241016_ImageClassifierFocus/Dataset/mergeChannels/concatenated_image_{i+1}.tif'\n",
        "    tiff.imwrite(output_path, concatenated)\n",
        "\n",
        "    # print(f\"Concatenated image saved at {output_path} with shape: {concatenated.shape}\")\n"
      ],
      "metadata": {
        "id": "-yLsnuzXwg6q"
      },
      "id": "-yLsnuzXwg6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "91655c7c",
      "metadata": {
        "id": "91655c7c"
      },
      "source": [
        "# 02 - Setting required parameters\n",
        "In this section, users can specify the necessary parameters to predict the segmentation mask for a given input image. The following parameters are required:\n",
        "\n",
        "**Model path**: The path to the trained model that will be used for segmentation prediction.\n",
        "\n",
        "**Input path**: The path to the folder containing the input images, or the path to a single 'tif' image.\n",
        "\n",
        "**Output path (Optional)**: The path where the output of the network will be saved. If you do not provide an output path, the algorithm will automatically create a folder named 'output' in the same folder as the input images, and save the predictions there.\n",
        "\n",
        "**Device**: The device that will be used to perform the operations.\n",
        "\n",
        "**Do not run this cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d63c199",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139,
          "referenced_widgets": [
            "571b3adcd41d4b07a2d0bab77f6dbd55",
            "eb11184f16b24fd5b932c06aeb41dc18",
            "b377531b60ac4f7e93aad027187c63a6",
            "7841ac2ac4ff48bfa225611c1a186237",
            "95a079e3be6f4c209076e98a7cafd6dc",
            "4c715d3c972f4b86b4eecdd99cff541e",
            "2f8bf9cb9d9040f6a3d3690780c17beb",
            "8e1ba322fa3c46fb9a04e22e19e80381",
            "3ab40a40e0704b30b6c1565431105d86",
            "183e13718bb64c6d81a20df3561bad16",
            "3215763ae9054d5a98363114ed45c095",
            "b646631c1e6740f2bc45542b3d814dba"
          ]
        },
        "id": "0d63c199",
        "outputId": "d30cd002-43c9-4a63-db35-4e7ba849dbc4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Model path:', layout=Layout(flex='1 1 auto', width='auto'), placeholder='Insert paâ¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "571b3adcd41d4b07a2d0bab77f6dbd55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Folder path:', layout=Layout(flex='1 1 auto', width='auto'), placeholder='Insert pâ¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7841ac2ac4ff48bfa225611c1a186237"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Output path:', layout=Layout(flex='1 1 auto', width='auto'), placeholder='Insert pâ¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f8bf9cb9d9040f6a3d3690780c17beb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Device: ', options=(('CPU', 'cpu'),), style=DescriptionStyle(description_width='initial'â¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "183e13718bb64c6d81a20df3561bad16"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "predict_interactive = PredictClassInteractive()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1bf4f4d",
      "metadata": {
        "id": "c1bf4f4d"
      },
      "source": [
        "# 03 - Do the prediction\n",
        "This line of code allows you to predict the images using the trained deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316bbf4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "316bbf4d",
        "outputId": "c1e5590b-a4ab-468e-83bd-ec34e0ffbfe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/image_classification_pytorch/core_code/util/deeplearning_util.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location= device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/20241016_ImageClassifierFocus/Models/.best_model_e14.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/image_classification_pytorch/core_code/predict.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  img = torch.tensor(input_img).unsqueeze(0).to(device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatenated_image_1.tif  DONE\n",
            "concatenated_image_2.tif  DONE\n",
            "concatenated_image_3.tif  DONE\n",
            "concatenated_image_4.tif  DONE\n",
            "concatenated_image_5.tif  DONE\n",
            "concatenated_image_6.tif  DONE\n",
            "concatenated_image_7.tif  DONE\n",
            "concatenated_image_8.tif  DONE\n",
            "concatenated_image_9.tif  DONE\n",
            "concatenated_image_10.tif  DONE\n",
            "concatenated_image_11.tif  DONE\n",
            "concatenated_image_12.tif  DONE\n",
            "concatenated_image_13.tif  DONE\n",
            "concatenated_image_14.tif  DONE\n",
            "concatenated_image_15.tif  DONE\n",
            "concatenated_image_16.tif  DONE\n",
            "concatenated_image_17.tif  DONE\n",
            "concatenated_image_18.tif  DONE\n",
            "concatenated_image_19.tif  DONE\n",
            "concatenated_image_20.tif  DONE\n",
            "concatenated_image_21.tif  DONE\n",
            "concatenated_image_22.tif  DONE\n",
            "concatenated_image_23.tif  DONE\n",
            "concatenated_image_24.tif  DONE\n",
            "concatenated_image_25.tif  DONE\n",
            "concatenated_image_26.tif  DONE\n",
            "concatenated_image_27.tif  DONE\n",
            "concatenated_image_28.tif  DONE\n",
            "concatenated_image_29.tif  DONE\n",
            "concatenated_image_30.tif  DONE\n",
            "concatenated_image_31.tif  DONE\n",
            "concatenated_image_32.tif  DONE\n",
            "concatenated_image_33.tif  DONE\n",
            "concatenated_image_34.tif  DONE\n",
            "concatenated_image_35.tif  DONE\n",
            "concatenated_image_36.tif  DONE\n",
            "concatenated_image_37.tif  DONE\n",
            "concatenated_image_38.tif  DONE\n",
            "concatenated_image_39.tif  DONE\n",
            "concatenated_image_40.tif  DONE\n",
            "concatenated_image_41.tif  DONE\n",
            "concatenated_image_42.tif  DONE\n",
            "concatenated_image_43.tif  DONE\n",
            "concatenated_image_44.tif  DONE\n",
            "concatenated_image_45.tif  DONE\n",
            "concatenated_image_46.tif  DONE\n",
            "concatenated_image_47.tif  DONE\n",
            "concatenated_image_48.tif  DONE\n",
            "concatenated_image_49.tif  DONE\n",
            "concatenated_image_50.tif  DONE\n",
            "concatenated_image_51.tif  DONE\n",
            "concatenated_image_52.tif  DONE\n",
            "concatenated_image_53.tif  DONE\n",
            "concatenated_image_54.tif  DONE\n",
            "concatenated_image_55.tif  DONE\n",
            "concatenated_image_56.tif  DONE\n",
            "concatenated_image_57.tif  DONE\n",
            "concatenated_image_58.tif  DONE\n",
            "concatenated_image_59.tif  DONE\n",
            "concatenated_image_60.tif  DONE\n",
            "concatenated_image_61.tif  DONE\n",
            "concatenated_image_62.tif  DONE\n",
            "concatenated_image_63.tif  DONE\n",
            "concatenated_image_64.tif  DONE\n",
            "concatenated_image_65.tif  DONE\n",
            "concatenated_image_66.tif  DONE\n",
            "concatenated_image_67.tif  DONE\n",
            "concatenated_image_68.tif  DONE\n",
            "concatenated_image_69.tif  DONE\n",
            "concatenated_image_70.tif  DONE\n",
            "concatenated_image_71.tif  DONE\n",
            "concatenated_image_72.tif  DONE\n",
            "concatenated_image_73.tif  DONE\n",
            "concatenated_image_74.tif  DONE\n",
            "concatenated_image_75.tif  DONE\n",
            "concatenated_image_76.tif  DONE\n",
            "concatenated_image_77.tif  DONE\n",
            "concatenated_image_78.tif  DONE\n",
            "concatenated_image_79.tif  DONE\n",
            "concatenated_image_80.tif  DONE\n"
          ]
        }
      ],
      "source": [
        "output = predict_interactive.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a65367e7",
      "metadata": {
        "id": "a65367e7"
      },
      "source": [
        "# 04 - Visualization\n",
        "This sections provides an opportunity for the user to inspect and visually analyze the results of the segmentation prediction. This step is important to ensure that the predicted segmentations are appropriate and accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa66d979",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "aa66d979",
        "outputId": "4a4acbee-5aba-4bc3-abef-ec7789152ca8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8285a9c72034>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_images_predicted_class_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted_class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ],
      "source": [
        "show_images_predicted_class_interactive(output[\"inputs\"], output[\"predicted_class\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "571b3adcd41d4b07a2d0bab77f6dbd55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Model path:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_eb11184f16b24fd5b932c06aeb41dc18",
            "placeholder": "Insert path here",
            "style": "IPY_MODEL_b377531b60ac4f7e93aad027187c63a6",
            "value": "/content/drive/MyDrive/20241016_ImageClassifierFocus/Models/.best_model_e14.pth"
          }
        },
        "eb11184f16b24fd5b932c06aeb41dc18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "1 1 auto",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "b377531b60ac4f7e93aad027187c63a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "7841ac2ac4ff48bfa225611c1a186237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Folder path:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_95a079e3be6f4c209076e98a7cafd6dc",
            "placeholder": "Insert path here",
            "style": "IPY_MODEL_4c715d3c972f4b86b4eecdd99cff541e",
            "value": "/content/drive/MyDrive/20241016_ImageClassifierFocus/Dataset/mergeChannels/"
          }
        },
        "95a079e3be6f4c209076e98a7cafd6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "1 1 auto",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "4c715d3c972f4b86b4eecdd99cff541e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "2f8bf9cb9d9040f6a3d3690780c17beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Output path:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8e1ba322fa3c46fb9a04e22e19e80381",
            "placeholder": "Insert path here",
            "style": "IPY_MODEL_3ab40a40e0704b30b6c1565431105d86",
            "value": ""
          }
        },
        "8e1ba322fa3c46fb9a04e22e19e80381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "1 1 auto",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "3ab40a40e0704b30b6c1565431105d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "183e13718bb64c6d81a20df3561bad16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CPU"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Device: ",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_3215763ae9054d5a98363114ed45c095",
            "style": "IPY_MODEL_b646631c1e6740f2bc45542b3d814dba"
          }
        },
        "3215763ae9054d5a98363114ed45c095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b646631c1e6740f2bc45542b3d814dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}